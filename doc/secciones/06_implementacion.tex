\chapter{Implementación}

En este capítulo se describirá en detalle la implementación del sistema y su diseño. Se comentarán las dificultades más relevantes que ha planteado el desarrollo y cómo se ha optado por resolverse. El repositorio\footnote{\url{https://github.com/pabloMillanCb/tfg}} del proyecto está dividido en cuatro directorios:

\begin{itemize}
    \item \textbf{editor/}: Contiene la aplicación web con el editor 3D.
    \item \textbf{visor/}: Aplicación Android para la reproducción de escenas.
    \item \textbf{backend/}: Servidor web que conecta las aplicaciones con la base de datos.
    \item \textbf{doc/}: Documentación del proyecto.
\end{itemize}

Se dedicará una sección a cada uno de los tres primeros directorios.


\section{Aplicación web}

Como ya se mencionó en capítulos anteriores, se usaría la biblioteca de \textit{React.js}\cite{react} para desarrollar la aplicación web. Para montar el proyecto se empleó \textbf{Vite}\cite{vite}, que permite crear un proyecto de React + TypeScript de forma automática.

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.50]{reactfolder}
    \caption[Estructura de directorios de React]{Estructura de directorios final de la aplicación web de React}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.50]{componentfolder}
    \caption[Componentes del proyecto React]{Lista de componentes del proyecto de la apliación web.}
\end{figure}

El bloque de construcción básico de las aplicaciones en React son los \textbf{componentes}. Estos son elementos funcionales de interfaz, reutilizables, anidables y parametrizables. Se escriben en la extensión de TypeScript \textit{.tsx}, y se encuentran en la carpeta \textit{src/components} del proyecto. Un componente puede ser por ejemplo un botón que haga una acción específica, o la cabecera de una página, un menú desplegable, etc. El corazón de la aplicación se encuentra en el componente \textbf{EditorComponent.tsx}, que gestiona el editor 3D y su interacción con el usuario.

\subsection{Renderización de entorno 3D}

Lo primero que se realizó, antes de implementar el componente en si con toda la interfaz, fue crear el entorno 3D interactuable. Para ello se creó la clase \textbf{EditorScene.ts}, que hereda de la clase \textit{Scene} de la librería de Three.js. La clase \textit{Scene} se encarga de gestionar como su nombre indica la escena entendida como el espacio donde habitan los distintos objetos 3D de nuestra aplicación. En Three.js, los espacios 3D se representan con una estructura de datos en forma de árbol. Los objetos son hijos unos de otros, y el objeto especial \textit{scene} es el nodo raíz. Un objeto puede establecerse como hijo de otro objeto a su vez, haciendo que todas las transformaciones que se apliquen al padre también lo hagan en el hijo.

Para poder visualizar un entorno 3D de Three.js se necesita un objeto \textit{Scene}, una cámara en forma de \textit{PerspectiveCámera} y un renderizador \textit{WebGLRenderer}. La cámara se puede entender conceptualmente como si fuera una cámara real. Es un objeto que está en una posición determinada del espacio 3D y apuntando en una dirección. Es la que se encarga de definir qué parte de la escena es la que se va a ver por pantalla. Esta información se transmite al renderizador, que se encarga de aplicar cálculos para convertir el espacio 3D de una representación abstracta a una imagen en dos dimensiones que se puede monstrar por pantalla y que el usuario puede entender. Los tres elementos se inicializan y mantienen desde \textbf{EditorSceneController.ts}. Esta clase hace de interfaz e implementa las llamadas necesarias para que el componente \textit{EditorComponent.tsx} pueda realizar cambios en la escena y cámara a través de su interfaz de usuario.

\subsection{Controles de cámara y selección de objetos}

Para la manipulación de la cámara se adoptaron controles estándares presentes en otros programas de edición: arrastrando con el click derecho del ratón se rota la cámara sobre un punto, y arrastrando con la rueda del ratón pulsada de desplaza. Así queda el click izquierdo libre para las interacciones con los objetos.

El usuario debe poder seleccionar un objeto de la escena colocando el ratón sobre y haciendo click. Esto le servirá para, una vez elegido un modelo, poder manipularlo moviéndolo, rotándolo o escalándolo sin afectar al resto de objetos de la escena. Esto presenta el problema de cómo saber cual es el objeto que se ha seleccionado. Para ello se utilizó un \textbf{raycaster}. Es un recurso muy usado en la informática gráfica y disponible en la librería de Three.js en el que se dispara una suerte de haz o rayo desde un punto de la escena y en una dirección. Este disparo puede atravesar o no objetos 3D que se encuentre en su camino. En caso afirmativo, devuelve una lista ordenada de los modelos con los que ha colisionado. Para este proyecto se ajustó un raycast desde el punto en el que se encuentra la cámara (es decir, la perspectiva del usuario) y en dirección al punto en el que se hace click. De esta forma el rayo ensarta el punto en el que se ha hecho click en el ratón, y si había un objeto debajo, colisionará con él. En la clase de raycast que implementa Three se le debe pasar como parámetro una \textit{Scene} o un \textit{Group}, un objeto especial que tiene como hijos una lista de objetos. El rayo solo detectará colisiones con esos objetos. Aparece así otro problema. Si se pasa la \textit{scene} como parámetro se detectaría cualquier objeto que haya en esta, no solo los que haya añadido el usuario.

Se debe distinguir distinguir entre dos tipos de objetos que se encontrarán en la escena: los dinámicos que los añade el usuario y son interactuables por este, y los estáticos, que se crean al iniciarse el espacio 3D automaticamente y tienen de propósito servir de referencia visual, como por ejemplo un \textit{grid} que simboliza dónde está el suelo. Todos estos objetos se deben añadir a la \textit{scene} como hijos de esta usando \textit{scene.add(object)}. Para diferenciar las dos clases de elementos se añade a la escena un \textit{Group} con el nombre de \textbf{liveObjects}. Los modelos introducidos por el usuario serán entonces añadidos como objetos a este atributo, y será el parámetro que se pasará al raycaster para que solo reconozca objetos \textit{dinámicos}, solucionando así el problema de reconocimiento. Para eliminar objetos de la escena simplemente habría que desconectar al nodo hijo de su padre.

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.30]{grid}
    \caption[Grid del editor de escenas]{Grid del editor de escenas para tener de referencia como suelo.}
\end{figure}

Sin embargo esto crea otro problema. Se ha comentado anteriormente que un objeto 3D puede ser hijo de otro, y esto es muchas veces el caso con los modelos que se pueden encontrar en la web. Es muy probable que si cargo en la escena a un robot, este esté hecho de múltiples objetos 3D. Uno de estos podría ser su brazo, su cabeza o su cuerpo. Esto es problemático porque si es el caso y hago click en el brazo del robot con la intención de seleccionar el modelo entero, el raycaster solo nos devolverá información sobre el nodo hijo. Para solucionar esto se hizo uso del atributo \textbf{name} que tienen todos los \textit{Object3D} de Three. Este atributo es un string con el que se puede asignar un nombre al objeto. En la función que añadía los objetos del usuario al grupo de \textbf{liveObjects}, se etiquetan los \textit{name} de todos los elementos como "alive". De esta forma, cuando se obtiene un nodo hijo de un modelo, se puede ir recorriendo el padre de cada nodo hasta encontrar uno cuyo \textit{name} sea igual a "alive". Aquí pararía la búsqueda ya que de seguir ascendiendo por sus padres se llegaría a la \textit{scene}, el nodo raíz.

Tras esto el raycaster sería capaz de detectar qué objeto selecciona el usuario. Este se almacenaría en una variable para poder aplicarle las transformaciones pertinentes cuando el usuario lo demande. Si este quisiera dejar de tenerlo seleccionado, deberá hacer click en cualquier punto de la pantalla en el que no haya ningún objeto. Si el raycaster colisiona con un total de 0 modelos, se liberará la variable del objeto seleccionado. Esta variable de la que se habla es en realidad un array de \textit{Object3D}, ya que el usuario tiene la posibilidad de seleccionar múltiples objetos para que las transformaciones que se apliquen afecten a todos simultáneamente. Para ello se hará click en otro objeto mientras se pulsa la tecla \textit{Control}. Este es un esquema de control usado tanto en software del sector como en los gestories de archivos de los sistemas operativos, es un gesto muy reconocido. Si ya había un objeto seleccionado y el usuario está pulsando la tecla mientras elige otro objeto, este se añadirá al array de objetos seleccionados. En caso contrario se vaciaría el array y se incluiría el nuevo objeto seleccionado.

Una vez está resuelta la selección de objetos es necesario implementar la forma en la que el usuario comanda las transformaciones que se aplican sobre estos. Se usarán los \textbf{gizmos} comentados en el anterior capítulo para ello. Three incorpora \textit{gizmos} de fábrica bajo la clase \textit{TransformControls}. Esta clase cuenta con tres modos: translación, rotación y escalado. La clase puede "engancharse" a un objeto que se le pase como argumento. Una vez hecho mostrará el gizmo correspondiente en función del modo actual superpuesto sobre el objeto. El usuario podrá hacer click directamente sobre él para manipular el objeto.

Aquí apareció otro problema. La clase solo soporta un objeto "enganchado" al mismo tiempo, por lo que en el caso de tener varios objetos seleccionados no se podrían modificar todos. La primera solución que se intentó dar a esto fue hacer a todos los modelos seleccionados hijos de un nodo temporal vacío el cual se asignaría al \textit{TransformControls}. Esto no funcionó debido a que las transformaciones en three son locales respecto al padre del objeto. Es decir, si se aglutinan los objetos seleccionados en un nodo y se mueven dentro de ese nodo, cuando se devuelven a la escena regresan a su lugar de origen. Esto se arregló con un \textit{dummie}, que es un objeto invisible que sirve como apoyo para hacer ciertas operaciones en la informática gráfica. En este caso, cuando se inicia una selección múltiple se crea un \textit{dummie} en el origen de cordenadas y se asigna a los \textit{TransformControls}. En cada frame, se comprueba cuanto se ha modificado su posición, rotación o escalado y se aplica esa misma variación a todos los objetos de la lista de seleccionados. Con esto la funcionalidad estaba lista.

\subsection{Reproducción de animaciones}

Algunos modelos 3D incluyen animaciones. El usuario cuenta con una opción en su interfaz para poder reproducir la escena, y con ello las animaciones de todos los objetos que se encuentren en esta. Los \textit{Object3D} de Three soportan animaciones, y para poder reproducirlas se hace uso de la clase \textbf{AnimationMixer}. Esta recibe un objeto como parámetro y automaticamente extrae toda la información relativa a sus animaciones. Cada animación está identificada con una string que le da nombre como podrían ser "Death", "Walk", "Run", etc. Desde \textit{AnimationMixer} se invoca a la función \textit{clipAction(animacion)} y se obtiene un objeto de tipo \textit{AnimationAction}. Desde este último se puede ordenar a las animaciones que se reproduzcan o se detengan.

Como el usuario necesita tener la capacidad de elegir una animación concreta de entre las muchas que puede tener un modelo para que se reproduzca, se necesita registrar en algún sitio cual es la seleccionada. Para ello se hará uso del atributo \textbf{userData} de \textit{Object3D}. Este es un objeto genérico JavaScript en el que se puede definir multitud de datos auxiliares relacionados con el modelo. Se define entonces el atributo \textbf{animationIndex}, un número entero que va de 0 a n-1, siendo n el número de animaciones que posee ese objeto. Cuando se quiere reproducir la animaciones en la escena, para cada objeto se crea un \textit{AnimationAction} con el índice actual del objeto y se llama al método \textit{animationClip.play()}. Cuando se quiera detener la animación, se ejecutará \textit{animationClip.reset()} para que los modelos vuelvan a su posición inicial y \textit{animationClip.stop()} para que se detengan.

\subsection{Exportación y carga de escenas}

Se procede ahora a hablar sobre la carga y exportación de modelos 3D. Se parte desde la base de que el usuario tiene acceso a archivos \textit{.glb} de forma local en su dispositivo, que lo ha cargado desde la interfaz de la aplicación, se guarda en un objeto JavaScript \textit{File} en la caché y con una url temporal para acceder al recurso (ya se entrará en detalles más adelante sobre esta parte del proceso). Para cargar el modelo se hará uso de la clase de Three.js \textbf{GLTFLoader}, que permite convertir un \textit{File} en \textit{Object3D} a partir de su url. Despues se añadiría el \textit{Object3D} como hijo a \textit{liveObjects} y ya sería visible en la escena.

Para la exportación de la escena es estrictamente necesario encontrar la forma de convertirla en un archivo único \textit{.glb}. Para ello se empleó la clase de THREE.js \textbf{GLTFExporter}. Esta recibe como argumento un objeto o grupo de objetos y los convierte a un archivo \textit{.gltf}, o \textit{.glb} si se especifica el parámetro \textit{binary} como verdadero. Como resultado se obtiene un buffer con un array de datos \textit{raw}, es decir binarios. Estos datos pueden usarse para crear un objeto \textit{Blob} que se puede descargarse como fichero a través del navegador. Solo es necesario entonces pasarle al \textbf{GLTFExporter} el grupo de objetos vivos (los que introdujo el usuario) \textit{liveObjects} y la clase hará el resto del trabajo. El archivo resultante no solo contendrá el modelado sino las animaciones de todos los objetos de la escena.

Con lo desarrollado hasta ahora sería posible descargar la escena al ordenador como usuario, pero existe un segundo caso en el que se requiere exportar una escena: guardarla en el servidor. Se necesita establecer la forma en la que se van a almacenar los datos de la escena como los objetos que contiene, su posición, rotación y escalado, su índice de animación activa, su lista de animaciones, etc. Para ello se contemplaron dos opciones.

\begin{itemize}
    \item \textbf{Archivo binario único}: Los modelos se exportarían a un fichero similar al descargable por el usuario en el que la escena se convierte en un único modelo. El archivo sería lo que se almacenaría en la base de datos, conteniendo este toda la información sobre los modelos que se tienen y su estado actual. Esto requeriría que fuera posible cargarlo a posteriori en el editor separando todos los modelos para añadirlos de forma individual a la escena y que conserven su independencia.
    
    \item \textbf{JSON}: Se guardaría en la base de datos una copia de cada modelo en la escena. Adicionalmente, se guardaría un archivo JSON en el que se describen uno por uno la posición, translación y escalado de cada objeto, además de otros parámetros como el \textit{animationIndex} para cada uno de ellos si es que tienen.
\end{itemize}

La primera opción era la preferible, ya que era una solución mucho más simple y elegante, tanto a la hora de empaquetar los datos como de almacenarlos en la base de datos (solo se guardaría un único archivo con toda la información). Por suerte, cuando \textit{GLTFLoader} carga un archivo \textit{.glb} conserva toda la estructura de nodos del objeto original. Tras hacer algunas pruebas se comprobó que \textit{GLTFExporter}, al recibir un grupo de objetos como entrada para exportar una escena, lo que hace es unirlos todos a un nodo raíz que sigue siendo accesible si volvemos a cargar el modelo exportado con \textit{GLTFLoader}. Por tanto se implementó una función \textit{loadScene} que recorría la lista de hijos del archivo \textit{.glb} recibido y los añadía uno a uno a la escena. 

\subsubsection{Conflicto entre animaciones}

Llegados a este punto la escena se cargaba correctamente pero surgió un problema derivado de las animaciones de los archivos exportados. Para explicar la resolución es necesario explicar algunos detalles sobre las animaciones en Three.js.

Las animaciones de un modelo se guardan en un atributo de \textit{Object3D} llamado \textbf{animations}, un array de objetos \textit{AnimationClip}. Estos objetos a su vez tienen entre sus atributos un array de \textit{KeyframeTrack}, objetos que almacenan secuencias de \textit{keyframes} con información sobre las transformaciones que realiza la animación a un objeto. ¿Cómo saben estas estructuras de datos a qué objeto se deben aplicar estos movimientos? \textbf{a través del \textit{name}} de los \textit{Object3D} de la escena.

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.55]{animationesquema}
    \caption[Esquema de estructura de datos para animaciones]{Esquema de las estructuras de datos de \textit{Object3D}, \textit{AnimationClip} y \textit{KeyframeTrack} para la reproducción de animaciones.}
\end{figure}


\textit{GLTFExporter} al realizar el empaquetamiento de la escena almacena todos los \textit{animations} de los objetos en un único array en el nodo raíz. Por lo tanto al cargar el archivo, no hay forma de saber qué animaciones pertenecían a cada objeto para poder asignarselas posteriormente. Para solucionar esto, antes de añadir un objeto a la escena se crea un array de datos en el atributo \textit{userData} de \textit{Object3D} conteniendo el nombre de todos los \textit{AnimationClip} del modelo. Así, al cargar cada objeto luego, se podría comprobar en la lista general de animaciones del \textit{.glb} y añadir a \textit{animations} las que coincidan en nombre.

Sabiendo todo esto se puede describir el verdadero problema en todo este asunto: \textbf{se producían conflictos al exportar y volver a cargar escenas con dos o más modelos idénticos con animaciones}. Este fue un punto crítico del desarrollo, ya que de arreglar esto dependía poder ahorrar luego mucho tiempo de desarrollo en el backend y simplificar el funcionamiento del sistema. Lo que se podía observar al generar este caso concreto es que al cargarse una escena previamente exportada con estas condiciones, solo el primer modelo de todos reproducía las animaciones correctamente.

La razón residía en \textit{GLTFExporter}, que cuando almacena la escena en un objeto tiene un comportamiento muy relevante para este caso: \textbf{si dos nodos tienen el mismo nombre, modifica uno de ellos}. Por ejemplo, si hubiera dos nodos llamados \textit{hand}, al segundo lo renombraría como \textit{$hand_1$}

Conociendo todo esto se puede intuir dónde está el error: si los modelos modelo tenía una animación que apuntaba al nodo \textit{hand}, solo se moverá el primer objeto, ya que los las animaciones del segundo apuntarán también a \textit{hand}, pero el nodo del segundo modelo en realidad se llama \textit{$hand_1$}, por lo que no se vé afectado por la animación.

Para solucionarlo se hizo uso de otro atributo de \textit{Object3D}, el \textbf{id}, una variable numérica que identifica un objeto en una escena. Al cargar nuevos objetos en la escena, se comprueba si coincide con alguno otro. En caso afirmativo, se concatena el \textit{id} del objeto recursivamente a todos sus nodos hijos, y luego se hace lo mismo para el array de \textit{animations}, para que los \textit{KeyframeTrack} apunten a los nodos con el nombre cambiado. Si el \textit{id} de un objeto repetido es \textit{23}, su nodo hijo \textit{hand} pasará a llamarse \textit{23\#hand}, evitando así todos los conflictos que puede generar el \textit{GLTFExporter}.

Con esto la exportación y carga de escenas es perfectamente funcional para cualquier caso, y podrá ser usada para almacenarse en la base de datos como un único archivo.

\subsection{Reproducción de audios}

Los audios asociados a las escenas se manejan desde \textbf{EditorSceneController.ts}. La clase tiene el atributo \textit{audio} de tipo \textit{HTMLAudioElement}, una clase que permite reproducir audios cargados por \textit{url}. Como se mencionó antes, se da por supuesto que el usuario carga un archivo de audio compatible y este se almacena en la caché accesible desde una url. Con la función \textit{loadAudio} se crea el objeto que gestionará su reproducción con los métodos \textit{playAudio} y \textit{stopAudio}.

\subsection{Interfaz del componente editor}

Ahora se discutirá el \textbf{EditorComponent.tsx} en si, el componente que se encarga de renderizar React y que implementa las funcionalidades del editor descrito hasta ahora con la interfaz para que el usuario pueda interactuar con la escena.

Todo lo que a la interfaz se refiere se diseño previamente en \textbf{Figma}\cite{figma}, un software para \textit{mockups} de interfaces de usuario. Todos los elementos como botones, selectores y cajas de texto se implementaron usando \textit{MUI Core}, una librería para React y JavaScript para la creación de interfaces de usuario. Ofrece todos bloques fundamentales que pueden necesitarse para la construcción de aplicaciones web como botones, selectores, scrolls, cajas de texto, iconos, etc. Además adopta \textit{Material}, unas directrices de diseño creadas por Google para interfaces web y móviles. Es lo que se acabaría usando en la aplicación Android, así que de entre todas las librerías que existían con el mismo propósito, se eligió esta para unificar en la medida de lo posible el estilo de las aplicaciones web y de móvil del proyecto.

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.19]{editorfinal}
    \caption[Interfaz de usuario para el editor de escenas]{Interfaz de usuario final para el editor de escenas.}
\end{figure}

La interfaz se dividió en tres secciones:

\begin{enumerate}
    \item La barra superior. Aquí se encontrarían los botones para exportar y guardar la escena, un cuadro de texto para introducir el nombre de la misma, y un botón para volver al anterior menú.
    \item La barra lateral, un panel de control con todas las opciones para configurar la escena AR.
    \item El \textit{canvas}, todo el espacio sobrante entre las dos barras donde se renderizaría el entorno 3D del editor.
\end{enumerate}

En este componente y en el resto de la aplicación se haría uso de los llamados \textit{hooks} de React. Estas son unas funciones JavaScript que entre otras cosas permiten dotar de estado a un componente, ya que de normal no tienen. Por ejemplo, si se quisiera hacer un contador de clicks, este tendría que tener el número de clicks actual guardado. Esto es posible gracias al hook de \textbf{useState}, que permite almacenar variables persistentes entre distintas renderizaciones de un componente. Para este proyecto se usa (por poner un ejemplo) para almacenar la instancia de \textit{EditorSceneController} que se crea al iniciar una escena por primera vez y renderizarlo en el \textit{canvas}. Como se ha señalado anteriormente, esta es la puerta de entrada del componente para manipular la escena. Cada vez que se active alguna opción para manipular la escena desde \textit{EditorComponent} lo comunicará llamando a la función pertinente de \textit{EditorSceneController}, como por ejemplo \textit{playAudio()}.

Para el panel de control lateral se tienen las siguientes utilidades:

\begin{itemize}
    \item \textbf{Botones de herramientas}: Con estos botones se puede elegir qué herramienta usar para manipular la escena. Dependiendo de cual se tenga escogida cambiará la forma de manipular un objeto. Cambiará de color en función de la herramienta seleccionada. Se cuenta con las siguientes:
        \begin{itemize}
            \item Translación: Con esta herramienta aparecerá un \textit{gimzo} de desplazamiento en los objetos seleccionados, con el que se podrá mover de posición.
            \item Rotación: De forma similar a la anterior herramienta, con esta se pueden rotar objetos.
            \item Escalado: Con esta herramienta se pueden hacer los objetos más grandes o más pequeños en cualquier eje de coordenadas.
            \item Papelera: Con esta herramienta seleccionada, los objetos clickados desaparecerán de la escena.
        \end{itemize}

    \item \textbf{Botón de play}: Previsualiza la escena en el editor. Reproduce las animaciones de los modelos de la escena si estos tuvieran y la opción de animaciones estuviera activa y/o el audio si hubiera.
    \item \textbf{Cargar modelo}: Abre el navegador de archivos del dispositivo para seleccionar un archivo \textit{.glb} y añadirlo a la escena. Opcionalmente el usuario puede cargar modelos arrastrándo el archivo a la ventana del navegador.
    \item \textbf{Cargar audio}: Abre el navegador de archivos del dispositivo para seleccionar un archivo de audio para asociarlo a la escena.
    \item \textbf{Animaciones}: Menú desplegable para establecer si reproducir o no las animaciones de los modelos.
    \item \textbf{Tipo de escena}: Menú desplegable para elegir el tipo de escena. Dependiendo del tipo elegido aparecerá abajo una opción adicional para añadir configuración adicional necesaria:
        \begin{itemize}
            \item Imágenes aumentadas (Marcador): Aparece un botón para cargar una imagen. Esta imagen se pasará a \textit{EditorScene} como objeto \textit{File} y será cargada como textura plana en el suelo de la escena para tenerla como referencia.
            \item Geoespacial: Aparecen tres campos numéricos para introducir la latitud, longitud y altura a la que se desea que aparezca la escena.
            \item Superficie: No se muestra ningún menú adicional.
        \end{itemize}
\end{itemize}

\subsection{Carga de archivos}

La carga de archivos como modelos, audio o imagen se gestiona a través de la función \textit{handleLoad}. Esta se llama en un evento \textit{onChange} para los elementos HTML asociados a los botones de carga. Este evento se activará cada vez que se cargue un nuevo archivo y lo pasará como parámetro en forma de objeto \textit{File}. Ahí se comprueba si es uno de los archivos soportados (\textit{.glb} para los modelos, \textit{.mp3} o \textit{.ogg} para los audios y \textit{.jpg}, \textit{.png} o \textit{.svg} para las imágenes). Si la extensión del archivo cae en alguna de estas categorías, se llama a la función de \textit{EditorSceneController} correspondiente: \textit{loadModel}, \textit{loadAudio} o \textit{loadImage}. La clase se encarga de gestionar esas entradas como se ha visto anteriormente.

\subsection{Gestión de evento asíncrono en el guardado}

Para guardar la escena se tienen en la esquina superior derecha dos botones, uno para descargar la escena como modelo (Exportar) y otro para guardarlo en el servidor (Guardar). Se debe tener en cuenta que la función de \textit{EditorScene} que convierte la escena en modelo, \textit{exportScene}, es asíncrona. En el caso de \textit{Exportar} no hay ningún problema, ya que cuando termina el proceso no hay que hacer nada más, es descargado por el navegador, pero en el caso de \textit{Guardar} surgía un inconveniente. Para enviar el archivo a la base de datos (se entrará en maś detalle sobre cómo en la sección del backend) se necesita esperar a que este termine. Por cómo funciona \textit{GLTFExporter}, el resultado de la función \textit{parse} (la que se usa para hacer la conversión) no puede ser devuelto por la función en la que se ejecuta ya que está dentro del contexto de una función que se pasa por argumento a \textit{parse}. Introducir en la clase de \textit{EditorScene} código para hacer peticiones HTTP al backend era posible pero una solución poco elegante ya que ese no es el propósito de la clase

Se optó por hacer en \textit{EditorScene} una versión clónica de \textit{exportScene()} llamada \textit{getBlob(upload: (blob: Blob) => void)}. Esta recibe como argumento una función, \textit{upload} que a su vez recibe como argumento un \textit{Blob} (equivalente a \textit{File}). Esta función es definida en \textit{EditorComponent} y llamada dentro del contexto de finalización de las labores de \textit{GLTFExporter}. Contiene el código para realizar las peticiones HTTP para enviar la escena al backend. En resumen, desde \textit{EditorScene} se ejecuta código de \textit{EditorComponent} para comunicarse con el servidor cuando la escena está lista para ser enviada.

\subsection{Navegación entre distintas páginas}

React es una librería y no un framework como se mencionó anteriormente. Por ello, de base solo tiene capacidad para soportar \textit{single page applications}. Sin embargo se necesitaba crear una interfaz con distintas páginas: inicio de sesión, configuración de perfil, selección de escena y editor. Para ello se empleó \textbf{React Router}\cite{reactrouter}, una librería que permite definir distintas páginas cada una asociada a una url distinta (\textit{web/login}, \textit{web/config}, etc). Se realizó un \textit{mockup} para la navegación del sitio web y sus interfaces.

Las páginas con las que cuenta la aplicación son las siguientes:

\begin{itemize}
    \item \textbf{MainPageComponent.tsx} \textit{/}: Página principal de los usuarios logeados. Se muestra el listado de escenas para poder editarlas o eliminarlas además de opciones para crear escena, cerrar sesión o cambiar credenciales.
    \item \textbf{SignInComponent} \textit{/login} y \textbf{SignUpComponent} \textit{/register}: Páginas para crear cuenta e iniciar sesión.
    \item \textbf{ConfigComponent} \textit{/config}: Menú para actualizar las credenciales del usuario.
    \item \textbf{EditorComponent} \textit{/editor}: Editor de escenas.
\end{itemize}

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.07]{editorfigma}
    \caption[Mockup de la UI de la aplicación web]{Mockup de la UI de la aplicación web realizado en \textit{Figma}}
\end{figure}

\subsection{Rutas protegidas}

Al ser accesible cualquier página de la aplicación surgía un nuevo problema. En el planteamiento realizado, el usuario debía registrarse o iniciar sesión antes de poder acceder a páginas como su lista de escenas, configuración o el editor (no tendría sentido de otra forma). Se necesitaba entonces \textit{proteger} las páginas sensibles al inicio de sesión bajo la condición de que se hubiera realizado. Esto se implementó con el componente \textbf{GuardedRoute.tsx}. Como se mencionó anteriormente, los componentes en React pueden estar anidados. Cada vez que se introduce un componente de alguna de las páginas de la aplicación, como podría ser \textit{EditorComponent} con dirección \textit{/editor}, se comprobaba si existe una sesión iniciada. En caso afirmativo renderiza a su hijo, y si no redirige la aplicación a \textit{/login} para que el usuario entre en la página de inicio de sesión. Para las páginas de inicio de sesión de registro (cada unas con su propio componente \textit{SignInComponent} y \textit{SignUpComponent}) se seguía el razonamiento contrario: si no existía un usuario logeado se renderizaban, y si no la aplicación se redirige a la página principal.

\subsection{Contextos compartidos}

En react cada componente tiene su propio contexto. Las funciones y variables que se declaran dentro de uno solo pueden ser accedidos por él mismo, a menos que un padre las pase como argumento a un hijo, en cuyo caso este sí  podría acceder. Sin embargo es un procedimiento un poco rudimentario si se va a necesitar más que una variable puntual, mucho más si hay componentes anidados a varios niveles y el nodo del fondo necesita hacer uso de algún elemento del padre de todos.

Esto fue un problema cuando se trataba con la gestión del usuario activo y llamadas a la API. El usuario activo se almacena en un objeto de la librería de \textit{Firebase} (se explicará en siguientes apartados). Este objeto debía instanciarse una vez y llamarse desde cualquier punto de la aplicación, lo cual era un problema por lo comentado sobre los contextos. Se tendría que pasar este objeto a cada elemento de la página que lo necesitase por parámetros, lo cual no era elegante.

Para solucionarlo se hizo uso de los \textit{createContext} de React. Estos permiten definir unos componentes especiales cuyos descendientes tienen acceso a las variables y funciones declaradas en el padre. \textit{userController} y \textit{sceneController} se crearon para manejar respectivamente las llamadas de usuarios y escenas. Tienen como hijos al resto de los componentes de la aplicación, por lo que se pueden llamar desde cualquier sitio. Además, encapsulan las funciones de comunicación con el servidor, por lo que si en algún momento se fuera a cambiar el backend del sistema solo habría que re escribir estos archivos.

\subsection{Animación de carga}

Existían muchos momentos en la aplicación que suponían una espera para el usuario, como por ejemplo cuando se obtiene el listado de escena, cuando se carga un modelo o al guardar la escena en el servidor. Para hacer saber al usuario que se están haciendo operaciones y la aplicación está reaccionando a sus órdenes se añadió una animación de carga de la librería de \textit{react-spinners}\cite{reactspinners}. Se creó un componente encargado de renderizar esta animación en el centro de la pantalla. En lugar de incluir este componente en cada página que necesitara de un proceso de carga (lo cual sería tedioso), se creó un nuevo \textit{context} de React que envolvía a toda la aplicación, haciendo que desde cualquier punto se pudiera manipular una variable booleana que activaba o desactivaba la animación de carga.

\subsection{Mensajes de incidencia y avisos}

Se introdujeron mensajes en las páginas de inicio de sesión, registro y cambio de datos para alertar al usuario el motivo del error a la hora de realizar una operación. Por ejemplo contraseña o correo incorrecto, clave poco segura, correo inválido o ya registrado, etc. También se añadieron mensajes de confirmación en acciones irreversibles como al eliminar una escena (\textit{¿seguro que desea eliminarla?}) o al abandonar el editor durante la creación de una escena (\textit{¿seguro? Se eliminarán los cambios no guardados}).

\section{Reproductor de escenas Android}

La aplicación Android se compone de tres elementos básicos: una página de inicio de sesión, una página con el listado de las escenas creadas por el usuario activo, y el visor de Realidad Aumentada en función de la escena seleccionada. Se planteó también en un principio una página de registro de usuario pero se descartó debido a que si una persona accedía al sistema por primera vez desde esta app no tendría ninguna escena creada para reproducir. Por tanto el registro de usuarios se mantendría unicamente en el editor web. Antes de comenzar con la programación se realizaron diseños para las distintas páginas de la aplicación y la navegabilidad entre estas. Estas páginas reciben el nombre de \textit{actividades} en Android Studio y se representan con una clase Kotlin y un archivo \textit{XML} para la interfaz.

%% figura navegación app android

\subsection{Archivo de configuración de escena}

Lo primero que hubo que definir es la estructura de datos con la que se representarían las escenas. Este sería el formato usado tanto para almacenarlos en memoria como para almacenarlos en la base de datos. Debido a que la base de datos de \textit{Firebase} funcionaba con objetos JSON se decidió que este fichero sería la forma de codificar las escenas. A continuación se tendría que decidir qué campos tendría que tener el archivo. Se llegó a la siguiente lista:

\begin{itemize}
    \item \textbf{name} \textit{(string)}: Nombre de la escena.
    \item \textbf{uid} \textit{(string)}: Identificador del usuario creador de la escena.
    \item \textbf{scene\_type} \textit{(string)}: Tipo de escena. Campos posibles: \textit{augmented\_images}, \textit{ground} o \textit{geospatial}.
    \item \textbf{model\_url} \textit{(string)}: Enlace de descarga del modelo.
    \item \textbf{loop} \textit{(boolean)}: De ser verdadero, las animaciones y audio se reproducirán en bucle.
    \item \textbf{audio} \textit{(string)}: Enlace de descarga del archivo de audio. Si esstá vacío es que no hay audio asociado.
    \item \textbf{image\_url} \textit{(string)}: Enlace de descarga de la imagen marcadora. Solo está relleno si la escena es de imágenes aumentadad.
    \item \textbf{coordinates} \textit{(float array)}: Coordenadas GPS en caso de que la escena sea geoespacial.
    \item \textbf{animations} \textit{(string array)}: Nombre de las animaciones que se reproducen en la escena.
\end{itemize}

\subsection{Escenas de imágenes aumentadas}

Todas los tipos de escena se reproducen en la misma actividad android, \textit{ArActivity.kt}. Cuando se inicia una escena se abre esta actividad y recibe como parámetro un JSON en forma de \textit{string}. Con la librería \textit{Gson}\cite{gson} se convierte en una \textit{data class} llamada \textbf{sceneParameters}, una clase sencilla que almacena únicamente una lista de atributos. Así se podrán acceder comodamente a lo largo de la ejecución. Lo primero que hace la actividad es comprobar el parámetro \textit{scene\_type} para saber qué tipo de escena se va a ejecutar.

Para configurar una escena de imágenes aumentadas en \textit{Sceneview} (la librería usada para manejar \textit{ARCore}), se debe crear un objeto de la clase \textit{ArSceneView}. A este objeto se le deberá indicar las distintas configuraciones en función del tipo de escena que se pretenda iniciar.

Como en este caso se iniciará una escena de imágenes aumentadas, se le pasa la función \textit{initializeSceneViewSession} donde se crea una base de datos de imágenes que deberán ser reconocidas por la aplicación. En este caso solo se tiene una imagen la cual se puede descargar en forma de \textit{Bitmap} con la url de \textit{sceneParameters}. Al realizar esta configuración, automáticamente se iniciará la cámara y se mostrará un recuadro blanco para enmarcar la imagen activadora de la escena.

Ahora se debe definir la función \textit{checkAugmentedImageUpdate} e introducirla en el objeto \textit{ArSceneView}. Esta función se ejecuta en cada fotograma e indicará qué hacer en el caso de encontrar con la cámara una imagen de las definidas previamente. Se escribirán aquí las llamadas necesarias a la biblioteca para enmarcar el modelo de la escena en la imagen. Para saber cómo se siguieron los ejemplos de la documentación.

primero se debe definir qué imágenes dispararán la escena. En este caso es una única imagen la cual se puede descargar en forma de \textit{Bitmap} con la url de \textit{sceneParameters}.

Para cargar el modelo en la escena solo es necesario instanciar un objeto de clase \textit{ArModelNode} con los siguientes parámetros:

\begin{itemize}
    \item \textbf{glbFileLocation}: Enlace de descarga del modelo. Se puede obtener de \textit{SceneParameters}. La clase se encargará de descargarlo automaticamente.
    \item \textbf{applyPoseRotation}: En caso de ser verdadero, aplica una rotación al modelo de la escena para que siempre se muestre apoyado en la imagen independientemente de la inclinación de esta.
    \item \textbf{scaleToUnits}: Multiplicador de escala que se le aplica al modelo. En un proceso de prueba y error se determinó que, para que el tamaño relativo del modelo a la imagen fuera lo más equivalente posible al del editor web, este parámetro tendría un valor de \textit{0.09}.
\end{itemize}

Este objeto se almacenará en un atributo de clase de la actividad y será el que se ancle a la posición de la imagen desde la función \textit{checkAugmentedImageUpdate}.

\subsection{Escenas por superficie}

Para este tipo de escenas se debe activar el flag de \textit{planeRenderer.isvisible}. Así se visualizará un patrón de puntos en los lugares que identifique la librería como suelos a través de la cámara. También hay que definir el \textit{planeFindingMode}, que determinará las estrategias de búsqueda de superficies que ejecuta \textit{SceneView} como por ejemplo buscar solo superficies horizontales, solo verticalez, fijar los puntos de anclaje a la mejor posición estimada, etc. Después de un proceso de prueba se determinó que el que mejores resultados obtenía era \textit{HORIZONTAL}.

%%revisar el planefindingmode

Se definieron dos botones en la interfaz, \textit{Cargar objeto} y \textit{Colocar objeto}. De primeras solo está el primer botón visible. Al pulsarlo, el modelo de la escena aparece en pantalla y comienza a colocarse a la superficie que esté apuntando el usuario. Si el usuario mueve la cámara, el objeto se desplaza acorde. Al pulsar el primer botón este desaparece y aparece el segundo. Cuando se pulsa este el objeto se queda fijo en la posición en la que se encontrara. Llegado a este punto el usuario puede mover el dispositivo como plazca, el objeto seguirá en el mismo punto.

%%cambiar nombre de botones

\subsection{Escenas geoespaciales}

Para emplear escenas geoespaciales era necesario habilitar la API Geoespacial\cite{geospatialapi} de ARCore en la aplicación. Esta es un servicio de \textit{Google Cloud} que provee entre otras cosas de funciones de geoposicionamiento para aplicaciones que usan ARCore. Para ello simplemente se siguieron las instrucciones indicadas en la página para habilitarlo y usarlo en una sesión de Realidad Aumentada.

Fue necesario habilitar una cuenta de \textit{Google Cloud} para el proyecto. Este es un servicio de pago pero que ofrece una versión gratuita con \$300 de presupuesto, suficientes para el desarrollo. Si se fuera a desplegar la aplicación a nivel comercial sería necesario introducir más presupuesto. El saldo se consume según el número de operaciones que se realicen con la API.

De vuelta al código de la aplicación con todo configurado, solo sería necesario crear un objeto de tipo \textit{Anchor} con las coordenadas GPS almacenadas en \textit{SceneParameters} y asignársela al nodo del modelo creado de forma similar a escenas explicadas anteriormente.

Un problema que surgió fue que el usuario no podía visualizar la escena a menos que estuviera muy cerca de esta. Para solventarlo se aumentó el atributo \textit{sceneView.cameraNode.farClipPlane}, que indica la distancia de renderizado máxima. Hay que tener en cuenta que las aproximaciones GPS en dispositivos móviles siempre son aproximadas hasta cierto punto, así que es posible que si se reproduce la misma escena varias veces el objeto aparezca en lugares ligeramente distintos. 

\subsection{Reproducción de audio y animaciones}

El audio se gestionó con la clase \textit{MediaPlayer}. Esta se encarga tanto de descargar el archivo a través de la \textit{URL} como de reproducirlo al indicárselo. Para las animaciones, la clase \textit{ArModelNode} contaba con soporte para reproducirlas. Solo era necesario indicar el nombre de las animaciones deseadas, los cuales se sacan de \textit{SceneParameters}.

El momento en el que se reproduce el audio y las animaciones. En imágenes aumentadas comienzan cuando se detecta la imagen activadora en \textit{checkAugmentedImageUpdate}. En el caso de las escenas por superficie es cuando se pulsa el botón de \textit{Colocar objeto}. Con las geospaciales simplemente comienzan cuando la escena se inicia.

\subsection{Menú de selección de escena}

Una vez se inicia sesión se muestra un menú \textit{scrolleable} en el que aparecen todas las escenas creadas por el usuario mostrando su nombre, el tipo de escena y un botón para iniciarla. También hay un botón para cerrar la sesión. Para implementar la lista de escena se hizo uso de un \textit{RecyclerView}. Este es un tipo de interfaz Android que permite la creación de menús con listas extensas de elementos repetidos. En este caso se repetiría la \textit{"tarjeta"} que representa cada una de las escenas en la interfaz. La peculiaridad que tiene \textit{RecyclerView} con otros elementos del estilo, es que solo crea los elementos que se encuentran en un instante determinado en la pantalla del dispositivo. Si el usuario se desplaza por el menú, creará los elementos nuevos que hayan entrado en pantalla y descartará los anteriores. Con esto se consigue que si un usuario ha creado un numero muy grande de escenas, el rendimiento de la aplicación no disminuya.

\section{Backend}

Para el backend se tienen dos elementos. Por un lado el servicio de \textit{Firebase}, hosteado por Google. Por otro lado se hizo un servidor web que hace de intermediario con ambas aplicaciones y \textit{Firebase}. Se van a detallar en los siguientes apartados las configuraciones y desarrollo que se realizó además de justificar las decisiones de diseño.

\subsection{Configuración de la base de datos}

Lo primero que se realizó fue crear un proyecto en \textit{Firebase}, el \textit{backend-as-a-service} que se utilizaría para la base de datos y autenticación de usuarios. Para ello se siguieron las instrucciones ofrecidas en la propia página. Se generaría una \textit{key} o clave que necesitaría cargarse en cualquier programa que hiciera uso directo de los servicios de \textit{Firebase}. El backend está hosteado \textit{24/7} por Google. Estos servicios tienen opciones de pago profesionales y una gratuita, que para propósitos del desarrollo sería suficiente. Esta simplemente limita la capacidad máxima para almacenar datos en la nube, el número de operaciones de lectura y escritura por día y funcionalidades varias de \textit{Google Cloud}.

Se procedió a configurar la base de datos. \textit{Firebase} utiliza una base de datos no relacional, que a diferencia de las convencionables no emplean tablas que necesiten definirse previamente. En su lugar aquí se tienen \textbf{colecciones}. Estas son conjuntos de \textbf{documentos}, los cuales contienen \textit{campos}. Los documentos tienen en esencia la misma forma que un archivo JSON, y de hecho es el formato que se emplea para el intercambio de información cuando se realizan peticiones a la base de datos. Se creó una colección llamada \textit{escenas} que contendría distintos documentos, cada uno de ellos almacenaría la información sobre una escena, con la misma estructura que se describió en la sección de la aplicación Android. En un inicio se creó otra colección para almacenar información de los usuarios, pero se descartó en favor del uso de \textit{Authentication}. Los documentos una vez creados son asignados automáticamente con un código identificador.

\subsection{Usuarios del sistema}

Uno de los servicios que ofrece \textit{Firebase} es \textbf{Authentication}. Este gestiona su propia base de datos de usuario. Los usuarios poseen un correo electrónico que usan para registrarse en el sistema, mantiene todas las contraseñas seguras bajo encriptación y genera automáticamenet \textit{ids} de usuario que los identifica inequivocamente. Acciones como registrarse como usuario, iniciar sesión, cambiar datos o gestionar los tokens autentificadores se realiza de forma automática y trasnparente al programador a través de simples llamadas a la librería de \textit{Firebase}. Es por eso que se decidió optar por esta forma de mantener la información de los usuarios en lugar de crear una colección en la base de datos.

\subsection{Almacenamiento de archivos binarios}

Otro de los servicios de \textit{Firebase} es \textbf{Storage}. Esta interfaz permite almacenar archivos binarios de tamaños elevados. Posteriormente se puede obtener una \textit{URL} a través de la cual descargar el fichero. Estos archivos pueden estar además almacenados en distintas carpetas. Para el proyecto se crearon las carpetas \textit{models}, \textit{audio} e \textit{images}. Como cada escena tendría como mínimo un archivo de modelo, y como mucho otro de imagen y de audio, el nombre de los ficheros almacenados sería la \textit{id} del documento de la escena. Así podrían rescatarse facilmente al obtener el JSON en la aplicación.

\subsection{Servidor Node.js}

Al igual que con la aplicación web, se usó \textit{Vite} para crear un proyecto base de JavaScript con \textit{Node.js}\cite{nodejs} y \textit{Express}\cite{express} para implementar la API a la que pedirían servicio las aplicaciones web y de Android. En el archivo \textit{index.js} se definen todas las llamadas que ofrece la interfaz. Cada llamada tiene una \textit{URL} asociada. Por ejemplo si se quiere subir una nueva escena, se haría desde la aplicación una petición HTTP POST a la dirección en la que esté alojado el servidor seguido de \textit{/post/escena}. Algunas llamadas tienen implícito en la dirección un parámetro, señalizado con \textit{":"} en la \textit{URL}. En el caso de \textit{/get/escenas/:id}, al hacer la petición se introduce en el \textit{:id} el identificador de usuario del que se quieran obtener las escenas.

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.60]{backendfolder}
    \caption[Estructura de archivos de backend]{Estructura de archivos del proyecto de Node.js para el backend}
\end{figure}

En la carpeta util se almacenan scripts que cargan objetos \textit{admin} y \textit{db} de \textit{Firebase} a través de los cuales se realizarán las llamadas a la base de datos. En la carpeta \textit{handlers} se encuentran las funciones correspondientes a las llamadas ya descritas. Aquí es donde se hace la conexión con \textit{Firebase} y se realizan operaciones adicionales. Cada petición tiene dos argumentos:

\begin{itemize}
    \item \textbf{req}, o request, es el objeto con la información de la petición. Aquí se almacenan los parámetros de la \textit{URL} o el \textit{body}, donde viene incluido archivos adjuntos si fuera el caso.
    \item \textbf{res}, o response, es la respuesta que se envía a la máquina que realizó la petición. Si se realizó una llamada para obtener una lista de escenas, esa información se añadirá en este objeto.
\end{itemize}

\subsection{Esquema y verificación de peticiones}

Se cuenta con tres agentes en la red: \textit{App}, hace referencia a la aplicación web o Android, el \textit{servidor Node.js} y \textit{Firebase}. Como se ha visto la aplicación realiza peticiones al servidor Node.js para obtener escenas. Sin embargo, en el caso de identificarse como usuario la aplicación conecta directamente con Firebase. Esto podría parecer anómalo pero tiene una razón detrás. Todo el tema de gestión de sesión de usuario viene resuelto ya en las librerías de Firebase para Android y JavaScript. Se tiene un objeto \textit{Authenticator} en memoria, y a través de este se puede crear un nuevo usuario, iniciar sesión, cambiar credenciales, comprobar si la sesión activa es válida, etc y todo ello de forma segura. Si se quisiera realizar todos esos cálculos desde el servidor Node.js añadiría muchos pasos extra y probablemente quedaría un resultado más pobre, ya que para este proyecto no se cuenta con la experiencia que puede tener un equipo de ingenieros de Google.

Pero si para el inicio de sesión se prescinde del servidor intermediario, ¿por qué no para todo lo demás? Por dos razones. La primera es que eso supondría programar todas las operaciones que realiza el servidor en una clase para React y Android Studio, lo cual sería redundante: repetiríamos código en dos proyectos distintos. Si en un futuro se quisiera actualizar alguna de estas funciones, tendría que hacerse por partida doble. Por otro lado teniendo el servidor intermedio obtenemos mayor modularidad. Al final su función es únicamente gestionar las peticiones que se realizan a la base de datos de escenas. Si en un futuro se decidiera sustituir esa base de datos por una opción más conveniente, pero manteniendo el \textit{Authenticator} de Firebase para la parte de usuarios, sería posible y facilmente implementable con la estructura que se propone.

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.50]{esquema-backend}
    \caption[Diagrama de comunicaciones del servidor]{Diagrama de comunicaciones del servidor}
\end{figure}

Para asegurar que el usuario que realiza las llamadas es realmente de quien dice ser, se hace uso de \textit{JWT} (\textit{JSON Web Tokens}). Es un estandar con el que se puede propagar entre dos partes y de forma segura la identidad de un usuario. En esencia es una cadena de texto codificada que puede enviarse junto a las peticiones para firmarlas verificando la identidad. El servicio \textit{Authentication} de Firebase genera un \textit{JWT} para cada usuario identificado que se puede obtener desde el cliente. En la implementación realizada, las aplicaciones envían al servidor por cada petición el token. Este es recibido por el servidor, y antes de atender la petición recibida, comprueba con \textit{Firebase} que el token es válido. Si es el caso, se atiende a la petición y se envía la respuesta. Los tokens tienen validez durante una hora. Una vez transcurrido ese tiempo, \textit{Firebase} genera uno nuevo el cual vuelve a ser obtenible desde la aplicación.

En el proyecto de Node.js se realiza esta comprobación en la función \textit{decodeToken} dentro del archivo \textit{middleware.js}. Esta comprobación ocurre a cada petición que llega. Si es exitosa comienza a ejecutarse el código que atiende la petición. Si no lo es, devuelve un mensaje de error y termina la comunicación.